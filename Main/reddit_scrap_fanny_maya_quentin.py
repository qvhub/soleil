# -*- coding: utf-8 -*-
"""reddit_scrap_fanny_maya_quentin.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XtpRZU3-afYh7bbxvklmcsEbb9UHIKA1

POUR LE README / REGROUPEMENT DES COMMENTAIRES
"""

##################################################################
#                       SCRAPPING REDDIT                         #
##################################################################
# utilisation de selenium pour gérer tout ce qui est javascript  #
# utilisation de BeautifulSoup pour extraire les données HTML    #
##################################################################

# installation des packages (selenium, chromedriver etc)

##################################################################
# les options pour faire fonctionner selenium sur colab

chrome_options = webdriver.ChromeOptions()
chrome_options.add_argument('--headless')
chrome_options.add_argument('--no-sandbox')
chrome_options.add_argument('--disable-dev-shm-usage')
wd = webdriver.Chrome('chromedriver',chrome_options=chrome_options)
driver =webdriver.Chrome('chromedriver',chrome_options=chrome_options)

# si utilisation ailleurs ou pas envie de mettre les options etc...

# PATH = "/Users/fb/chromedriver"  # le chemin vers le chromedriver(ou autre)
# driver = webdriver.Chrome#(PATH)  # moi j'ai mis chrome mais vouspouvez mettre Firefox etc

##################################################################

# obtention de l'url de la page initiale 

##################################################################

# FONCTIONS POUR PRENDRE LES INFOS DE LA PAGE (avec beautifulsoup) :
# on prend la page source via le driver et on en fait une bonne soupe pour pouvoir utiliser BeautifulSoup
# selection de la partie de la page qui englobe toutes les infos que l'on veut obtenir

# Fonctions pour les infos à scraper (date, titre, nombre de points, nombre de commentaires)

##################################################################

# utilisation de selenium pour cliquer à la page suivante
# verification que le bouton pour passer à la page suivante existe
# si le texte correspond on continue à aller à la page suivante et à scraper, sinon c'est la fin
# mise à jour de l'url après avoir cliqué sur next, pour obtenir les infos de la page suivante


##################################################################

# enregistrement des données dans un csv

"""# S E T U P"""

!pip install selenium
!apt-get update 
!apt install chromium-chromedriver

from selenium import webdriver
from bs4 import BeautifulSoup
import time
import pandas as pd
from random import randint
import seaborn as sns

##################################################################
# les options pour faire fonctionner selenium sur colab

chrome_options = webdriver.ChromeOptions()
chrome_options.add_argument('--headless')
chrome_options.add_argument('--no-sandbox')
chrome_options.add_argument('--disable-dev-shm-usage')
wd = webdriver.Chrome('chromedriver',chrome_options=chrome_options)
driver =webdriver.Chrome('chromedriver',chrome_options=chrome_options)

#PATH = "/Users/fb/chromedriver"  # le chemin vers le chromedriver(ou autre)
#driver = webdriver.Chrome#(PATH)  # moi j'ai mis chrome mais vouspouvez mettre Firefox etc

"""# S U J E T : "Mercredi Tech"

Pour le sujet "Mercredi Tech" dans r/france : 
ça fonctionne, le code arrive à récupérer tous les sujets sur toutes les pages (4 pages)
utilisation du old.reddit au lieu du reddit actuel car c'est + facile à scraper
"""

##################################################################
#                       SCRAPPING REDDIT                         #
##################################################################
# utilisation de selenium pour gérer tout ce qui est javascript  #
# utilisation de BeautifulSoup pour extraire les données HTML    #
##################################################################

date = []
title = []
point = []
comment = []

##################################################################

# l'url de la page initiale (le sujet Mercredi Tech dans r/france)
url = driver.get("https://old.reddit.com/r/france/search?q=Mercredi+Tech&restrict_sr=on&sort=new&t=all")

print(driver.title)  # print le titre


##################################################################
# FONCTIONS POUR PRENDRE LES INFOS DE LA PAGE

def scrape(my_url):
    # pour montrer l'url actuelle
    print(driver.current_url)
    time.sleep(randint(3, 5))  # fait dodo pour éviter le ban
    # on prend la page source via le driver et on en fait une bonne soupe pour pouvoir utiliser BeautifulSoup
    html = driver.page_source
    soup = BeautifulSoup(html, "html.parser")

    # un peu tout ce qui englobe les infos que l'on veut extraire
    # c'est a partir de là qu'on va les chercher plus précisément
    articles_reddit = soup.find_all('div', class_='search-result')

    # infos à scrapper
    def infos():

        # pour trouver la date
        def nb_date():
            for i in articles_reddit:
                day = i.find('span', class_='search-time').time['datetime']
                date.append(day)
            print(date)

        nb_date()

        # pour trouver les titres
        def nb_titre():
            for i in articles_reddit:
                name = i.div.header.a.text
                title.append(name)
            print(title)

        nb_titre()

        # pour trouver les scores
        def nb_score():
            for i in articles_reddit:
                score = i.find('span', class_='search-score').text
                point.append(score)
            print(point)

        nb_score()

        # pour trouver les commentaires
        def nb_comment():
            for i in articles_reddit:
                com = i.find('a', class_='search-comments').text
                comment.append(com)
            print(comment)

        nb_comment()

    infos()

    # pour cliquer à la page suivante
    next_click = driver.find_elements_by_xpath("/html/body/div[4]/div[2]/div/footer/div/span/a")[-1]
    # texte du bouton (prev ou next)
    next_text = driver.find_elements_by_xpath("/html/body/div[4]/div[2]/div/footer/div/span/a")[-1].text
    # si le texte correspond on continue à aller à la page suivante et à scraper, sinon c'est la fin
    if next_text == 'suivant ›' or next_text == 'next ›':  # 'suivant ›' ou 'next ›' ça dépend de vous
        next_click.click()
        url = driver.get(driver.current_url) # mise à jour de l'url après avoir cliqué
        scrape(url)
    else:
        print('------------------------THE END---------------------------')
        driver.quit()


scrape(url)


##################################################################
#                           THE END                              #
##################################################################

# enregistrement des données dans un csv

df = pd.DataFrame(data={"date": date, "title": title, "points": point, "comments": comment})
df.to_csv("tech_reddit.csv", sep=',',index=False)


dfreddit = pd.read_csv('tech_reddit.csv')

dfreddit.shape

"""# S U B R E D D I T : r/jeuxvideo

Pour r/jeuxvideo, les infos ne sont pas à la même place que quand on fait une recherche par mot clé, donc il y a des choses à modifier dans le code
"""

##################################################################
#                       SCRAPPING REDDIT                         #
##################################################################
# utilisation de selenium pour gérer tout ce qui est javascript  #
# utilisation de BeautifulSoup pour extraire les données HTML    #
##################################################################

date = []
title = []
point = []
comment = []

##################################################################


# l'url de la page initiale (le subreddit r/jeuxvideo)
url = driver.get("https://old.reddit.com/r/jeuxvideo/")

print(driver.title)  # print le titre


##################################################################
# FONCTIONS POUR PRENDRE LES INFOS DE LA PAGE

def scrape(my_url):
    # pour montrer l'url actuelle
    print(driver.current_url)
    time.sleep(randint(3, 5))  # fait dodo pour éviter le ban
    # on prend la page source via le driver et on en fait une bonne soupe pour pouvoir utiliser BeautifulSoup
    html = driver.page_source
    soup = BeautifulSoup(html, "html.parser")

    # un peu tout ce qui englobe les infos que l'on veut extraire
    # c'est a partir de là qu'on va les chercher plus précisément
    articles_reddit = soup.find_all('div', class_='thing')
    #print(articles_reddit)

    # infos à scrapper
    def infos():

        # pour trouver la date
        def nb_date():
            for i in articles_reddit:
                day = i.find('p', class_='tagline').time['datetime']
                date.append(day)
            print(date)

        nb_date()

        # pour trouver les titres
        def nb_titre():
            for i in articles_reddit:
                #name = i.div.header.a.text
                name = i.find('a', class_='title').text
                title.append(name)
            print(title)

        nb_titre()

        # pour trouver les scores
        def nb_score():
            for i in articles_reddit:
                score = i.find('div', class_='score unvoted').text
                point.append(score)
            print(point)

        nb_score()

        # pour trouver les commentaires
        def nb_comment():
            for i in articles_reddit:
                com = i.find('a', class_='comments').text
                comment.append(com)
            print(comment)

        nb_comment()

    
    infos()
    time.sleep(randint(3,4))


    # pour cliquer à la page suivante
    
    print(driver.find_element_by_class_name("next-button").text)

    try:
      
      next_click = driver.find_element_by_class_name("next-button")
      next_click.click()
      url = driver.get(driver.current_url) # mise à jour de l'url après avoir cliqué
      scrape(url)

    except:

      print('------------------------THE END---------------------------')
      driver.quit()

   
        


scrape(url)

# enregistrement des données dans un csv

df = pd.DataFrame(data={"date": date, "title": title, "points": point, "comments": comment})
df.to_csv("jeux_video_reddit.csv", sep=',',index=False)


df_jv_reddit = pd.read_csv('jeux_video_reddit.csv')

df_jv_reddit.shape

"""# S U B R E D D I T : r/france

Pour r/france, les infos ne sont pas à la même place que quand on fait une recherche par mot clé, donc il y a des choses à modifier dans le code 


Nous avons récupéré 950 articles pour r/jeuxvideo et 938 pour f/france, il doit possiblement y avoir une limite d'article visibles. Ceux pour r/france datent tous de 2021
"""

##################################################################
#                       SCRAPPING REDDIT                         #
##################################################################
# utilisation de selenium pour gérer tout ce qui est javascript  #
# utilisation de BeautifulSoup pour extraire les données HTML    #
##################################################################

date = []
title = []
point = []
comment = []

##################################################################


# l'url de la page initiale (le subreddit r/france)
url = driver.get("https://old.reddit.com/r/france/")

print(driver.title)  # print le titre


##################################################################
# FONCTIONS POUR PRENDRE LES INFOS DE LA PAGE

def scrape(my_url):
    # pour montrer l'url actuelle
    print(driver.current_url)
    time.sleep(randint(3, 5))  # fait dodo pour éviter le ban
    # on prend la page source via le driver et on en fait une bonne soupe pour pouvoir utiliser BeautifulSoup
    html = driver.page_source
    soup = BeautifulSoup(html, "html.parser")

    # un peu tout ce qui englobe les infos que l'on veut extraire
    # c'est a partir de là qu'on va les chercher plus précisément
    articles_reddit = soup.find_all('div', class_='thing')
    #print(articles_reddit)

    # infos à scrapper
    def infos():

        # pour trouver la date
        def nb_date():
            for i in articles_reddit:
                day = i.find('p', class_='tagline').time['datetime']
                date.append(day)
            print(date)

        nb_date()

        # pour trouver les titres
        def nb_titre():
            for i in articles_reddit:
                #name = i.div.header.a.text
                name = i.find('a', class_='title').text
                title.append(name)
            print(title)

        nb_titre()

        # pour trouver les scores
        def nb_score():
            for i in articles_reddit:
                score = i.find('div', class_='score unvoted').text
                point.append(score)
            print(point)

        nb_score()

        # pour trouver les commentaires
        def nb_comment():
            for i in articles_reddit:
                com = i.find('a', class_='comments').text
                comment.append(com)
            print(comment)

        nb_comment()

    time.sleep(randint(3,4))
    infos()
    time.sleep(randint(3,4))


    # pour cliquer à la page suivante
    
    print(driver.find_element_by_class_name("next-button").text)
    
    try:
      
      next_click = driver.find_element_by_class_name("next-button")
      next_click.click()
      url = driver.get(driver.current_url) # mise à jour de l'url après avoir cliqué
      scrape(url)

    except:

      print('------------------------THE END---------------------------')
      driver.quit()

   
        
scrape(url)

# enregistrement des données dans un csv

df = pd.DataFrame(data={"date": date, "title": title, "points": point, "comments": comment})
df.to_csv("france_reddit.csv", sep=',',index=False)


df_france_reddit = pd.read_csv('france_reddit.csv')

df_france_reddit.shape

#CHROME code version optimisée (pas fini)

##################################################################

date = []
title = []
point = []
comment = []

##################################################################


# l'url de la page initiale (le subreddit r/france)
url = driver.get("https://old.reddit.com/r/france/")

print(driver.title)  # print le titre


##################################################################
# LES FONCTIONS SECONDAIRES

# Recupere Titre des articles
def nb_titre(i, articles_reddit):
    # name = i.div.header.a.text
    name = i.find('a', class_='title').text
    title.append(name)
    print(title)


# Charge les articles en auto
def js_page(driver):
    # pour cliquer à la page suivante
    #next_click = driver.find_elements_by_xpath("/html/body/div[4]/div/div[59]/span/span/a")[-1]
    next_click = driver.find_element_by_class_name("next-button")
    # texte du bouton (prev ou next)
    #next_text = driver.find_elements_by_xpath("/html/body/div[4]/div/div[59]/span/span/a")[-1].text
    next_text= driver.find_element_by_class_name("next-button").text
    # si le texte correspond on continue à aller à la page suivante et à scraper, sinon c'est la fin
    if next_text == 'suivant ›' or next_text == 'next ›':  # 'suivant ›' ou 'next ›' ça dépend de vous
        next_click.click()
        url = driver.get(driver.current_url) # mise à jour de l'url après avoir cliqué
        scrape(url)
    else :
        print('------------------------THE END---------------------------')
        driver.quit()

##################################################################
# FONCTION PRINCIPALE
# FONCTIONS POUR PRENDRE LES INFOS DE LA PAGE

def scrape(my_url):
    # pour montrer l'url actuelle
    print(driver.current_url)
    time.sleep(randint(3, 5))  # fait dodo pour éviter le ban
    # on prend la page source via le driver et on en fait une bonne soupe pour pouvoir utiliser BeautifulSoup
    html = driver.page_source
    soup = BeautifulSoup(html, "html.parser")

    # un peu tout ce qui englobe les infos que l'on veut extraire
    # c'est a partir de là qu'on va les chercher plus précisément
    articles_reddit = soup.find_all('div', class_='thing')
    #print(articles_reddit)

    # Ici la grande loop pour consulter chaque article
    for i in articles_reddit:
        nb_titre(i, articles_reddit)

    js_page(driver)

scrape(url)

"""# S U J E T : not working

## sujet : "ordi"

Pour le sujet "ordi" dans r/france, le même code arrive à récupérer les infos ( date, titre, nombre de points et commentaires), mais quand il clique sur suivant pour la dernière page, il  n'y a apparemment plus de sujets et il y a comme texte 'apparemment il n'y a rien ici". 
Cela fait une IndexError car il n'y a plus de bouton prev/next, juste le texte cité. (8 pages)
"""

##################################################################
#                       SCRAPPING REDDIT                         #
##################################################################
# utilisation de selenium pour gérer tout ce qui est javascript  #
# utilisation de BeautifulSoup pour extraire les données HTML    #
##################################################################

date = []
title = []
point = []
comment = []

##################################################################


# l'url de la page initiale (le sujet ordi dans r/france)
url = driver.get("https://old.reddit.com/r/france/search?q=ordi&restrict_sr=on&sort=new&t=all")

print(driver.title)  # print le titre


##################################################################
# FONCTIONS POUR PRENDRE LES INFOS DE LA PAGE

def scrape(my_url):
    # pour montrer l'url actuelle
    print(driver.current_url)
    time.sleep(randint(3, 5))  # fait dodo pour éviter le ban
    # on prend la page source via le driver et on en fait une bonne soupe pour pouvoir utiliser BeautifulSoup
    html = driver.page_source
    soup = BeautifulSoup(html, "html.parser")

    # un peu tout ce qui englobe les infos que l'on veut extraire
    # c'est a partir de là qu'on va les chercher plus précisément
    articles_reddit = soup.find_all('div', class_='search-result')

    # infos à scrapper
    def infos():

        # pour trouver la date
        def nb_date():
            for i in articles_reddit:
                day = i.find('span', class_='search-time').time['datetime']
                date.append(day)
            print(date)

        nb_date()

        # pour trouver les titres
        def nb_titre():
            for i in articles_reddit:
                name = i.div.header.a.text
                title.append(name)
            print(title)

        nb_titre()

        # pour trouver les scores
        def nb_score():
            for i in articles_reddit:
                score = i.find('span', class_='search-score').text
                point.append(score)
            print(point)

        nb_score()

        # pour trouver les commentaires
        def nb_comment():
            for i in articles_reddit:
                com = i.find('a', class_='search-comments').text
                comment.append(com)
            print(comment)

        nb_comment()

    infos()


    # pour cliquer à la page suivante
    next_click = driver.find_elements_by_xpath("/html/body/div[4]/div[2]/div/footer/div/span/a")[-1]
    # texte du bouton (prev ou next)
    next_text = driver.find_elements_by_xpath("/html/body/div[4]/div[2]/div/footer/div/span/a")[-1].text
    # si le texte correspond on continue à aller à la page suivante et à scraper, sinon c'est la fin
    if next_text == 'suivant ›' or next_text == 'next ›':  # 'suivant ›' ou 'next ›' ça dépend de vous
        next_click.click()
        url = driver.get(driver.current_url) # mise à jour de l'url après avoir cliqué
        scrape(url)
    else:
        print('------------------------THE END---------------------------')
        driver.quit()


scrape(url)

"""## sujet : "paris"

Pour le sujet "paris" dans r/france:

premier problème : à la 6eme page, ça fait une AttributeError: 'NoneType' object has no attribute 'a' (pour la fonction pour récupérer les titres)
J'ai donc mis la fonction nb_titre en commentaire pour essayer d'executer sans les titres et voir si il arrive à récup les autres infos sur toutes les pages

deuxieme problème : on voit que ça s'arrête à la page 10 et qu'il n'y a plus de bouton "next", alors que le dernier sujet date d'à peine 1 mois. Les gens n'ont pas commencé à parler de Paris dans r/france qu'à partir d'il y a un mois, on suppose donc que quand on fait une recherche de mot clé, c'est limité à 10 pages max.
"""

##################################################################
#                       SCRAPPING REDDIT                         #
##################################################################
# utilisation de selenium pour gérer tout ce qui est javascript  #
# utilisation de BeautifulSoup pour extraire les données HTML    #
##################################################################

date = []
title = []
point = []
comment = []

##################################################################


# l'url de la page initiale (le sujet Paris dans r/france)
url = driver.get("https://old.reddit.com/r/france/search?q=paris&restrict_sr=on&sort=new&t=all")

print(driver.title)  # print le titre


##################################################################
# FONCTIONS POUR PRENDRE LES INFOS DE LA PAGE

def scrape(my_url):
    # pour montrer l'url actuelle
    print(driver.current_url)
    time.sleep(randint(3, 5))  # fait dodo pour éviter le ban
    # on prend la page source via le driver et on en fait une bonne soupe pour pouvoir utiliser BeautifulSoup
    html = driver.page_source
    soup = BeautifulSoup(html, "html.parser")

    # un peu tout ce qui englobe les infos que l'on veut extraire
    # c'est a partir de là qu'on va les chercher plus précisément
    articles_reddit = soup.find_all('div', class_='search-result')

    # infos à scrapper
    def infos():

        # pour trouver la date
        def nb_date():
            for i in articles_reddit:
                day = i.find('span', class_='search-time').time['datetime']
                date.append(day)
            print(date)

        nb_date()

        # pour trouver les titres
        #def nb_titre():
            #for i in articles_reddit:
                #name = i.div.header.a.text
                #title.append(name)
            #print(title)

        #nb_titre()

        # pour trouver les scores
        def nb_score():
            for i in articles_reddit:
                score = i.find('span', class_='search-score').text
                point.append(score)
            print(point)

        nb_score()

        # pour trouver les commentaires
        def nb_comment():
            for i in articles_reddit:
                com = i.find('a', class_='search-comments').text
                comment.append(com)
            print(comment)

        nb_comment()

    infos()


    # pour cliquer à la page suivante
    next_click = driver.find_elements_by_xpath("/html/body/div[4]/div[2]/div/footer/div/span/a")[-1]
    # texte du bouton (prev ou next)
    next_text = driver.find_elements_by_xpath("/html/body/div[4]/div[2]/div/footer/div/span/a")[-1].text
    # si le texte correspond on continue à aller à la page suivante et à scraper, sinon c'est la fin
    if next_text == 'suivant ›' or next_text == 'next ›':  # 'suivant ›' ou 'next ›' ça dépend de vous
        next_click.click()
        url = driver.get(driver.current_url) # mise à jour de l'url après avoir cliqué
        scrape(url)
    else:
        print('------------------------THE END---------------------------')
        driver.quit()


scrape(url)

"""# [S C R A P I N G] What's next ? TO DO :

- Trouver les solutions pour les sujets "ordis" et "paris" afin de renforcer l'efficacité du code et palier à + de problèmes possibles
- Continuer à optimiser le code commencé pour r/france
- Essayer de voir si on peut contourner la limite du nombre d'articles et de pages
- Essayer de scraper le reddit actuel pour voir les différences avec old.reddit
- (Faire des scripts universels qui fonctionnent pour toutes les pages) 
- rendre le code + sécurisé (pour éviter le ban) et + rapide à executer
"""